{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import requests.exceptions \n",
    "from urllib.parse import urlsplit\n",
    "from urllib.request import Request\n",
    "from urllib.request import urlopen\n",
    "from flask import request\n",
    "from collections import deque \n",
    "import re \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_driver():\n",
    "    \n",
    "    global driver\n",
    "    #driver = webdriver.Chrome()\n",
    "\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    wait = WebDriverWait(driver, 7)\n",
    "\n",
    "    driver.get('https://www.verif.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(raisons,cas,formes,capitaux,codes,gerants,descriptions):\n",
    "    for ii in range(1,25):\n",
    "        try:\n",
    "            line = driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[1]'.format(ii)).text.lower()\n",
    "            \n",
    "            ## RAISON SOCIALE\n",
    "            if \"raison sociale\" in line:\n",
    "                raisons.append(driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[2]'.format(ii)).text)\n",
    "            \n",
    "            ## CA\n",
    "            if \"chiffre d'affaire\" in line:\n",
    "                cas.append(driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[2]'.format(ii)).text.split('\\n')[0])\n",
    "            \n",
    "            ## FORME JURIDIQUE\n",
    "            if \"forme juridique\" in line:\n",
    "                formes.append(driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[2]'.format(ii)).text.split('\\n')[0])\n",
    "            \n",
    "            ## CAPITAL SOCIAL\n",
    "            if \"capital social\" in line:\n",
    "                capitaux.append(driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[2]'.format(ii)).text.split('\\n')[0])\n",
    "            \n",
    "            ## APE / NAF\n",
    "            if 'ape' in line or 'naf' in line:\n",
    "                codes.append(driver.find_element_by_xpath('//*[@class=\"table infoGen hidden-smallDevice\"]/tbody/tr[{}]/td[2]'.format(ii)).text.split('\\n')[0])\n",
    "                           \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    ## GERANT(S)\n",
    "    try:\n",
    "        gerants.append(driver.find_element_by_xpath('//*[@class=\"table table-default dirigeants\"]').text.replace('\\n',' - '))\n",
    "    except:\n",
    "        gerants.append('error')\n",
    "        \n",
    "    ## DESCRIPTION\n",
    "    try:\n",
    "        descriptions.append(driver.find_element_by_xpath('//*[@class=\"accroche\"]').text.replace('\\n',' ; '))\n",
    "    except:\n",
    "        descriptions.append('error')\n",
    "    \n",
    "    for arr in [raisons,cas,formes,capitaux,codes,gerants,descriptions] :\n",
    "        if len(arr) < len(max( [raisons,cas,formes,capitaux,codes,gerants,descriptions] , key = lambda x : len(x))):\n",
    "            arr.append('error')\n",
    "    \n",
    "    return raisons,cas,formes,capitaux,codes,gerants,descriptions   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.vetementpro.com/content/2-mentions-legales']\n",
      "1\n",
      "Processing https://www.vetementpro.com/content/2-mentions-legales\n",
      "['539871657', '721525133', '104734598', '160510667']\n",
      "['https://www.protextyl.com/content/8-mentions-legales']\n",
      "1\n",
      "Processing https://www.protextyl.com/content/8-mentions-legales\n",
      "['13807006360', '531801587', '719210149', '159076348', '160510667']\n",
      "['https://www.oxwork.com/mentions-legales']\n",
      "1\n",
      "Processing https://www.oxwork.com/mentions-legales\n",
      "['531173615', '160458228', '075311736', '160458424', '864000000']\n",
      "['https://www.protecthoms.com/mentions-legales/']\n",
      "1\n",
      "Processing https://www.protecthoms.com/mentions-legales/\n",
      "['390145563', '723901455', '012345678', '504731036', '303325320']\n",
      "                        Website        Siren Raison sociale structure  cas  \\\n",
      "0   https://www.vetementpro.com    539871657                      NaN  NaN   \n",
      "1   https://www.vetementpro.com    721525133                      NaN  NaN   \n",
      "2   https://www.vetementpro.com    104734598                      NaN  NaN   \n",
      "3   https://www.vetementpro.com    160510667                      NaN  NaN   \n",
      "4     https://www.protextyl.com  13807006360                      NaN  NaN   \n",
      "5     https://www.protextyl.com    531801587                      NaN  NaN   \n",
      "6     https://www.protextyl.com    719210149                      NaN  NaN   \n",
      "7     https://www.protextyl.com    159076348                      NaN  NaN   \n",
      "8     https://www.protextyl.com    160510667                      NaN  NaN   \n",
      "9        https://www.oxwork.com    531173615                      NaN  NaN   \n",
      "10       https://www.oxwork.com    160458228                      NaN  NaN   \n",
      "11       https://www.oxwork.com    075311736                      NaN  NaN   \n",
      "12       https://www.oxwork.com    160458424                      NaN  NaN   \n",
      "13       https://www.oxwork.com    864000000                      NaN  NaN   \n",
      "14  https://www.protecthoms.com    390145563                      NaN  NaN   \n",
      "15  https://www.protecthoms.com    723901455                      NaN  NaN   \n",
      "16  https://www.protecthoms.com    012345678                      NaN  NaN   \n",
      "17  https://www.protecthoms.com    504731036                      NaN  NaN   \n",
      "18  https://www.protecthoms.com    303325320                      NaN  NaN   \n",
      "\n",
      "   codes capitaux formes gerants Elements number  \n",
      "0    NaN      NaN    NaN     NaN             NaN  \n",
      "1    NaN      NaN    NaN     NaN             NaN  \n",
      "2    NaN      NaN    NaN     NaN             NaN  \n",
      "3    NaN      NaN    NaN     NaN             NaN  \n",
      "4    NaN      NaN    NaN     NaN             NaN  \n",
      "5    NaN      NaN    NaN     NaN             NaN  \n",
      "6    NaN      NaN    NaN     NaN             NaN  \n",
      "7    NaN      NaN    NaN     NaN             NaN  \n",
      "8    NaN      NaN    NaN     NaN             NaN  \n",
      "9    NaN      NaN    NaN     NaN             NaN  \n",
      "10   NaN      NaN    NaN     NaN             NaN  \n",
      "11   NaN      NaN    NaN     NaN             NaN  \n",
      "12   NaN      NaN    NaN     NaN             NaN  \n",
      "13   NaN      NaN    NaN     NaN             NaN  \n",
      "14   NaN      NaN    NaN     NaN             NaN  \n",
      "15   NaN      NaN    NaN     NaN             NaN  \n",
      "16   NaN      NaN    NaN     NaN             NaN  \n",
      "17   NaN      NaN    NaN     NaN             NaN  \n",
      "18   NaN      NaN    NaN     NaN             NaN  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "import requests.exceptions\n",
    "\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "from urllib.request import Request\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from flask import request\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import re\n",
    "\n",
    "import http.cookiejar\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "from pyxlsb import open_workbook as open_xlsb\n",
    " \n",
    "\n",
    "# a queue of urls to be crawled\n",
    "\n",
    "#new_urls = deque(['https://www.ubaldi.com/accueil/'])\n",
    "df = []\n",
    "df_output=pd.DataFrame(columns=['Website','Siren','Raison sociale structure','cas','codes','capitaux','formes','gerants','Elements number'])\n",
    "\n",
    "with open_xlsb('test.csv') as wb:\n",
    "    with wb.get_sheet(1) as sheet:\n",
    "        for row in sheet.rows():\n",
    "            df.append([item.v for item in row])\n",
    "\n",
    "df = pd.DataFrame(df[1:], columns=df[0])\n",
    "\n",
    "list_URL=df['Website']\n",
    "integer=0\n",
    "for i in list_URL:\n",
    "\n",
    "    URL=(str(\"https://www.\")+i)\n",
    "    #URL='https://www.protextyl.com/'\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "           'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "           'Accept-Encoding': 'none',\n",
    "           'Accept-Language': 'en-US,en;q=0.8',\n",
    "           'Connection': 'keep-alive'}\n",
    "    # a set of urls that we have already crawled\n",
    "\n",
    "    processed_urls = set()\n",
    "\n",
    "\n",
    "\n",
    "    # a set of crawled emails\n",
    "\n",
    "    SIRET = []\n",
    "    SIREN= []\n",
    "\n",
    "\n",
    "    def TestAccessible(the_uri):\n",
    "\n",
    "    # l'URL du site web est valide, on va chercher s'il existe un lien vers la page contact et retourner ce lien\n",
    "\n",
    "        #html_page = urlopen(the_uri)\n",
    "        req = urllib.request.Request(the_uri, headers=hdr)\n",
    "        opener = urllib.request.build_opener()\n",
    "        response = opener.open(req)\n",
    "        try:\n",
    "            html_page = urllib.request.urlopen(req)\n",
    "        except urllib.HTTPError as e:\n",
    "            print(e.fp.read())\n",
    "        #response is now a string you can search through containing the page's html\n",
    "\n",
    "        soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "        LinkList = []\n",
    "\n",
    "        for Link in soup.find_all('a'):\n",
    "\n",
    "\n",
    "\n",
    "            LinkFound=Link.get('href')\n",
    "\n",
    "            if LinkFound is not None:\n",
    "\n",
    "                if \"http\" in LinkFound:\n",
    "\n",
    "                    LinkList.append(str(LinkFound))\n",
    "\n",
    "                else:\n",
    "\n",
    "                    LinkList.append(str(the_uri+LinkFound))\n",
    "\n",
    "\n",
    "\n",
    "        contactList = list(filter(lambda x: \"mentions-legales\" in x,LinkList))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return (contactList)\n",
    "\n",
    "\n",
    "\n",
    "    new_urls=TestAccessible(URL)\n",
    "\n",
    "    print(new_urls)\n",
    "\n",
    "    print(len(new_urls))\n",
    "\n",
    "    i=0\n",
    "\n",
    "\n",
    "\n",
    "    # process urls one by one until we exhaust the queue\n",
    "\n",
    "    while i<len(new_urls):\n",
    "\n",
    "        new_url=deque([new_urls[i]])\n",
    "\n",
    "        # move next url from the queue to the set of processed urls\n",
    "\n",
    "        url = new_url.popleft()\n",
    "\n",
    "        processed_urls.add(url)\n",
    "\n",
    "\n",
    "\n",
    "        # extract base url to resolve relative links\n",
    "\n",
    "        parts = urlsplit(url)\n",
    "\n",
    "        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "\n",
    "        path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "\n",
    "\n",
    "        # get url's content\n",
    "\n",
    "        print(\"Processing %s\" % url)\n",
    "\n",
    "        try:\n",
    "\n",
    "            response = requests.get(url)\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "\n",
    "            i+=1\n",
    "\n",
    "            # ignore pages with errors\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        requete = requests.get(url)\n",
    "\n",
    "        page = requete.content\n",
    "\n",
    "        soup = BeautifulSoup(page)\n",
    "\n",
    "        #SIRET_find = set(re.findall(r'[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]+\\s[0-9][0-9][0-9][0-9][0-9]', soup.text, re.I))\n",
    "        #if len(SIRET_find)!=0:\n",
    "            #for sir in SIRET_find:\n",
    "                #SIRET.append(sir) \n",
    "        \n",
    "        #SIRET_find = set(re.findall(r'[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9][0-9][0-9][0-9]', soup.text, re.I))\n",
    "        #if len(SIRET_find)!=0:\n",
    "            #for sir in SIRET_find:\n",
    "                #SIRET.append(sir) \n",
    "\n",
    "        #SIRET_find = set(re.findall(r'[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', soup.text, re.I))\n",
    "        #if len(SIRET_find)!=0:\n",
    "            #for sir in SIRET_find:\n",
    "                #SIRET.append(sir) \n",
    "\n",
    "        #SIRET_find = set(re.findall(r'^[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9]', soup.text, re.I))\n",
    "        #if len(SIRET_find)!=0:\n",
    "            #for sir in SIRET_find:\n",
    "                #SIRET.append(sir) \n",
    "\n",
    "\n",
    "\n",
    "        SIREN_find = set(re.findall(r'[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]+\\s[0-9][0-9][0-9]', soup.text, re.I)) \n",
    "        if len(SIREN_find)!=0:\n",
    "            for sir in SIREN_find:\n",
    "                sir=sir.replace(\" \",\"\")\n",
    "                SIREN.append(sir) \n",
    "            \n",
    "        SIREN_find = set(re.findall(r'[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]', soup.text, re.I)) \n",
    "        if len(SIREN_find)!=0:\n",
    "            for sir in SIREN_find:\n",
    "                SIREN.append(sir) \n",
    "\n",
    "\n",
    "    #taille=max(len(SIREN),len(SIRET))\n",
    "    taille=len(SIREN)\n",
    "    \n",
    "    while len(SIREN)!=taille:\n",
    "        SIREN.extend([''])\n",
    "        \n",
    "    #while len(SIRET)!=taille: \n",
    "        #SIRET.extend([''])\n",
    "        \n",
    "    for j in range(0,taille):\n",
    "       \n",
    "        df_output.loc[integer,\"Website\"]=URL\n",
    "        df_output.loc[integer,\"Siren\"]=SIREN[j]\n",
    "        #df_output.loc[integer,\"Siret\"]=SIRET[j]\n",
    "        \n",
    "        integer+=1\n",
    "        \n",
    "    \n",
    "\n",
    "    print(SIREN)\n",
    "\n",
    "    \n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19  - -  average time : 4.57  - -  error  - -  error  - -  error  - -  error  - -  error  - -  error\n",
      "                        Website      Siren Raison sociale structure  \\\n",
      "0   https://www.vetementpro.com  539871657               COMMERSENS   \n",
      "5     https://www.protextyl.com  531801587                   MEYDES   \n",
      "9        https://www.oxwork.com  531173615                  PROSHOP   \n",
      "14  https://www.protecthoms.com  390145563             PROTECT'HOMS   \n",
      "\n",
      "             cas                                              codes  \\\n",
      "0    4 925 311 €     4791A / Vente à distance sur catalogue général   \n",
      "5          error  4642Z / Commerce de gros (commerce interentrep...   \n",
      "9      130 118 €  4791B / Vente à distance sur catalogue spécialisé   \n",
      "14  21 502 633 €  4649Z / Commerce de gros (commerce interentrep...   \n",
      "\n",
      "       capitaux                          formes  \\\n",
      "0      81 000 €                            SARL   \n",
      "5      45 000 €                            SARL   \n",
      "9       2 000 €  Société par action simplifiées   \n",
      "14  1 059 000 €  Société par action simplifiées   \n",
      "\n",
      "                                              gerants  Elements number  \n",
      "0   Gérant M LAFONTAINE Emmanuel - Gérant M GERARD...              6.0  \n",
      "5                               Gérant M MEYER Franck              5.0  \n",
      "9   Président M VAN ISACKER Franck - Directeur gén...              6.0  \n",
      "14  Président PROCOGEST FINANCES - Directeur finan...              6.0  \n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "init_driver()\n",
    "\n",
    "raisons = []\n",
    "cas = []\n",
    "formes = []\n",
    "capitaux = []\n",
    "codes = []\n",
    "gerants = []\n",
    "descriptions = []\n",
    "start = time.time()\n",
    "nan_cols=[]\n",
    "for i,siren in enumerate(df_output[\"Siren\"]):\n",
    "    \n",
    "    try:\n",
    "        driver.get('https://www.verif.com/societe/' + str(siren))\n",
    "    except:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            init_driver()\n",
    "            driver.get('https://www.verif.com/societe/' + str(siren))\n",
    "        except:\n",
    "            next(sirens)\n",
    "            driver.get('https://www.verif.com/societe/' + str(siren))\n",
    "                  \n",
    "    get_info(raisons,cas,formes,capitaux,codes,gerants,descriptions)\n",
    "        \n",
    "    step = time.time()\n",
    "    avgtime = (step - start)/len(raisons)\n",
    "                    \n",
    "    clear_output()\n",
    "    print(len(raisons) , ' - - ' , 'average time : {0:.2f}'.format(avgtime), ' - - ' , raisons[-1] , ' - - ' , cas[-1] , ' - - ' , formes[-1] , ' - - ' , capitaux[-1] , ' - - ' , codes[-1] , ' - - ' , gerants[-1])\n",
    "    df_output['Raison sociale structure'][i] = raisons[-1]\n",
    "    df_output['cas'][i] = cas[-1]\n",
    "    df_output['codes'][i] =  codes[-1]\n",
    "    df_output['capitaux'][i] = capitaux[-1]\n",
    "    df_output['formes'][i] =  formes[-1] \n",
    "    df_output['gerants'][i] =  gerants[-1] \n",
    "    col_output=['Raison sociale structure','cas','codes','capitaux','formes','gerants']\n",
    "    \n",
    "    df_output['Elements number'][i]=len([col for col in col_output if df_output[col][i]!=\"error\"])\n",
    "\n",
    "    \n",
    "df_output['Elements number'] = df_output['Elements number'].replace(0, np.nan)\n",
    "df_output = df_output.dropna(axis=0, subset=['Elements number'])\n",
    "\n",
    "print(df_output)\n",
    "\n",
    "\n",
    "if os.path.exists(\"output.xlsx\"):\n",
    "    os.remove(\"output.xlsx\")\n",
    "else:\n",
    "    print(\"The file does not exist\")\n",
    "\n",
    "writer_2 = pd.ExcelWriter('output.xlsx')\n",
    "# write dataframe to excel\n",
    "df_output.to_excel(writer_2)\n",
    "# save the excel\n",
    "writer_2.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
